"""
server.py — FastAPI server for the Local RAG Knowledge API.

Start with:
    python server.py

Endpoints:
    GET  /          Health check
    POST /query     Retrieve relevant chunks (no generation)
    POST /ask       Retrieve chunks + generate an answer with Ollama

Interactive API docs (auto-generated by FastAPI):
    http://localhost:8000/docs
"""

from pathlib import Path

import chromadb
import ollama
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import uvicorn


# --- Config ---
CHROMA_DIR   = Path(__file__).parent / "chroma_db"
COLLECTION   = "game_knowledge"
EMBED_MODEL  = "all-MiniLM-L6-v2"
OLLAMA_MODEL = "llama3.2:3b"
TOP_K        = 3   # number of chunks to retrieve per query


# ---------------------------------------------------------------------------
# App setup
# ---------------------------------------------------------------------------

app = FastAPI(
    title="Local RAG Knowledge API",
    description="RAG retrieval server. Returns relevant text chunks and optionally generates answers via Ollama.",
    version="2.0.0",
)


# ---------------------------------------------------------------------------
# Load model and DB at startup (once, not on every request)
# ---------------------------------------------------------------------------

print(f"Loading embedding model '{EMBED_MODEL}'...")
embed_model = SentenceTransformer(EMBED_MODEL)
print("  Model ready.")

if not CHROMA_DIR.exists():
    raise RuntimeError(
        f"ChromaDB directory not found at '{CHROMA_DIR}'. "
        "Run 'python ingest.py' first to build the vector database."
    )

print(f"Connecting to ChromaDB at '{CHROMA_DIR}'...")
chroma_client = chromadb.PersistentClient(path=str(CHROMA_DIR))
collection = chroma_client.get_collection(COLLECTION)
print(f"  Connected. Collection has {collection.count()} chunks.")
print("\nServer ready. Starting uvicorn...\n")


# ---------------------------------------------------------------------------
# Request / Response models
# ---------------------------------------------------------------------------

class QueryRequest(BaseModel):
    question: str

    model_config = {
        "json_schema_extra": {
            "examples": [
                {"question": "What are the P.E.K.K.A's weaknesses?"},
                {"question": "What is the relationship between the Giant and the Witch?"},
                {"question": "Who counters the Hog Rider?"},
            ]
        }
    }


class RetrievedChunk(BaseModel):
    text:   str
    source: str
    score:  float


class QueryResponse(BaseModel):
    question: str
    results:  list[RetrievedChunk]


class AskResponse(BaseModel):
    question: str
    answer:   str
    sources:  list[RetrievedChunk]  # the chunks used to generate the answer


# ---------------------------------------------------------------------------
# Shared retrieval helper
# ---------------------------------------------------------------------------

def retrieve(question: str) -> tuple[list[RetrievedChunk], list[str]]:
    """
    Embed the question and fetch the top K chunks from ChromaDB.

    Returns
    -------
    chunks : list[RetrievedChunk]   — structured results for the response
    texts  : list[str]              — raw text strings for building the LLM prompt
    """
    query_embedding = embed_model.encode(question).tolist()

    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=TOP_K,
        include=["documents", "metadatas", "distances"],
    )

    documents = results["documents"][0]
    metadatas = results["metadatas"][0]
    distances = results["distances"][0]

    chunks = [
        RetrievedChunk(
            text   = doc,
            source = meta["source"],
            score  = round(1 - dist, 4),
        )
        for doc, meta, dist in zip(documents, metadatas, distances)
    ]

    return chunks, documents


# ---------------------------------------------------------------------------
# Endpoints
# ---------------------------------------------------------------------------

@app.get("/", summary="Health check")
def health_check():
    """Returns server status and how many chunks are indexed."""
    return {
        "status":         "ok",
        "chunks_indexed": collection.count(),
        "embed_model":    EMBED_MODEL,
        "ollama_model":   OLLAMA_MODEL,
    }


@app.post("/query", response_model=QueryResponse, summary="Retrieve relevant chunks (no generation)")
def query(request: QueryRequest):
    """
    Embed the question and return the top matching chunks from the knowledge base.
    No LLM generation — raw retrieval only.
    """
    question = request.question.strip()
    if not question:
        raise HTTPException(status_code=400, detail="Question cannot be empty.")

    chunks, _ = retrieve(question)
    return QueryResponse(question=question, results=chunks)


@app.post("/ask", response_model=AskResponse, summary="Retrieve + generate an answer with Ollama")
def ask(request: QueryRequest):
    """
    Retrieve the top matching chunks, then pass them to Ollama as context
    to generate a natural language answer.

    Steps:
    1. Embed the question and retrieve top K chunks (same as /query)
    2. Build a prompt: context chunks + question
    3. Send to Ollama (llama3.2:3b running locally)
    4. Return the generated answer alongside the source chunks
    """
    question = request.question.strip()
    if not question:
        raise HTTPException(status_code=400, detail="Question cannot be empty.")

    # Step 1: retrieve
    chunks, raw_texts = retrieve(question)

    # Step 2: build prompt
    # We give the model the retrieved chunks as context and instruct it to
    # answer only from that context. This keeps answers grounded in our docs.
    context = "\n\n".join(raw_texts)
    prompt = f"""You are a helpful assistant. Answer the question using the context below.
Keep your answer direct and try to make it one sentence long. If it's not clear from the context, you may infer but indicate in your response.

Context:
{context}

Question: {question}

Answer:"""

    # Step 3: call Ollama
    # ollama.chat() sends a message to the locally running Ollama server.
    # It blocks until the model finishes generating.
    response = ollama.chat(
        model=OLLAMA_MODEL,
        messages=[{"role": "user", "content": prompt}],
    )
    answer = response.message.content.strip()

    # Step 4: return answer + the source chunks so the caller can see
    # what context was used to generate the response
    return AskResponse(question=question, answer=answer, sources=chunks)


# ---------------------------------------------------------------------------
# Entry point
# ---------------------------------------------------------------------------

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
